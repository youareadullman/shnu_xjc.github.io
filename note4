# 数据挖掘笔记4  

> @author：徐佳诚    

逻辑回归与最大熵模型

**逻辑回归模型的学习过程**：通过求解对数似然函数极大化，得到条件概率分布 $P(Y|X)$。

**逻辑回归模型的预测过程**：直接使用模型的条件概率分布 $P(Y|X)$ 进行预测。

**逻辑斯谛回归模型的类别划分**：

- 用于解决分类问题的监督学习模型
- 概率模型&非概率模型：既可以看作是概率模型，也可以看作是非概率模型
- 参数化模型：假设模型参数的维度固定
- 判别模型：由数据直接学习决策函数 $f(Y|X)$

**逻辑斯谛回归模型的主要优点**：直接对分类进行建模，无需事先假设数据分布；在预测类别的同时，还可以得到近似概率预测

**逻辑斯谛回归模型的主要缺点**：不太适合特征空间很大的情况。

---

**最大熵模型的学习过程**：通过求解对数似然函数极大化或对偶函数极大化，得到条件概率分布 $P(Y|X)$。

**最大熵模型的预测过程**：直接使用模型的条件概率分布 $P(Y|X)$ 进行预测。

**最大熵模型的类别划分**：

- 用于解决分类问题的监督学习模型
- 判别模型：由数据直接学习决策函数 $f(Y|X)$

**最大熵模型的主要优点**：可以灵活地选择特征，使用不同类型的特征。

**最大熵模型的主要缺点**：计算量巨大。

---

【补充说明】逻辑回归模型，也称逻辑斯谛回归模型、LR 模型。



## 6.1 逻辑回归模型

> **【重点】逻辑回归模型的含义**
>
> 在逻辑斯蒂回归模型中，输出 $Y=1$ 的对数几率是输入 x 的线性函数。或者说，输出 $Y=1$ 的对数几率是由输入 x 的线性函数表示的模型，即逻辑斯蒂回归模型。

#### Sigmoid 函数的性质

$$
f(x) = \frac{1}{1+e^{-x}}
$$

其性质如下：

- 定义域为 $O(-\infty,+\infty)$，值域为 $(0,1)$，在定义域内连续，严格单调递增；
- 以 $(0,\frac{1}{2})$ 中心对称；
- 其一阶导数为 $f'(x) = f(x) (1-f(x))$；
- 具有简单变形 $f(x)=\frac{e^x}{1+e^x}$（分子分母同乘 $e^x$）。

#### 为什么要用 sigmoid 函数？



对于二类分类问题，有输入变量 $x \in R^n$ 和输出变量 $y \in \{0,1\}$。其基本的线性回归的形式为：

$$
y = w^T x + b
$$

其中 $w \in R^n$ 是权值向量，$b \in R$ 是偏置。显然，线性回归产生的预测值是一个连续变量。

很自然地，我们首先考虑通过线性回归直接预测正类的概率，即 $P(Y=1|x)$。但是线性回归的预测值的值域是 $(-\infty,+\infty)$，而我们希望这个预测值的值域是 $(0,1)$，于是我们需要修改线性回归预测的内容。

首先，我们考虑将线性回归改为预测事件的几率，即该事件发生的概率与该事件不发生的概率的比值，即 $\frac{P(Y=1|x)}{1-P(Y=1|x)}$。此时，我们希望预测值的值域变为 $(0,+\infty)$。

考虑到在进行分类时，我们是通过比较两个条件概率值的大小决定分类的；因此，在线性回归外套一个单调递增函数并不会影响分类结果；同时，为便于计算，我们还希望外套的这个函数任意阶可导。于是，可以选择外套底数大于 1 的指数函数，不妨先取底数为 $e$：

$$
\frac{P(Y=1|x)}{1-P(Y=1|x)} = e^{w^T x + b}
$$

此时，线性回归拟合的就是事件对数几率，即

$$
log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w^T x + b
$$

综上所述，以上过程就是在寻找比正类概率更适合线性回归拟合的目标，而找到的结果，就是逻辑回归模型。

#### 似然函数


> **【如何理解似然函数? - Yeung Evan 的回答 - 知乎】摘要**
>
> 似然函数定义为给定样本值 $x$ 关于未知参数 $\theta$ 的函数，是样本集中各个样本的联合概率：
>
> $$
> L(\theta|x) = f(x|\theta)
> $$
>
> 其中，$x$ 是输入变量 $X$ 取到的值，即 $X=x$；$\theta$ 为未知参数，$f(x|\theta)$ 是给定 $\theta$ 的情况下 $x$ 的联合密度函数。
>
> 似然函数 $L(\theta|x)$ 是关于 $\theta$ 的函数，密度函数 $f(x|\theta)$ 是关于 $x$ 的函数，两者仅在函数值上相等，而定义域和对应关系并不相等。
>
> 似然函数 $L(\theta|x)$ 表示给定样本 $X=x$ 下参数 $\theta$ 为真实值的可能性；密度函数 $f(x|\theta)$ 表示给定 $\theta$ 下样本随机向量 $X=x$ 的可能性。

## 6.2 最大熵模型

> **【重点】最大熵原理（来自《统计学习方法》P.95）**
>
> 直观地，最大熵原理认为要选择的概率模型必须满足已有的事实，即约束条件。在没有更多信息的情况下，那么不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值指标。

#### 单纯形

单纯形是三角形和四面体的一种泛化，即凸多面体；$k$ 维单纯形是指包含 $(k+1)$ 个结点的凸多面体。

#### 经验分布

当分布由样本数据估计得到时，所对应的联合分布 $P(X,Y)$ 和边缘分布 $P(X)$ 就是联合分布 $P(X,Y)$ 的经验分布和边缘分布 $P(X)$ 的经验分布。

#### 约束条件的含义是什么？

对于约束条件公式：

$$
\sum_{x,y} \tilde{P}(x) P(y|x) f(x,y) = \sum_{x,y} \tilde{P}(x,y) f(x,y)
$$

我们可以将其理解为，模型必须完全服从所有 $f(x,y)=1$ 的实例点的概率总和，但对所有 $f(x,y)=1$ 的实例点之间不做区分。


#### 最大熵模型为什么可以通过对偶问题求解？

关于拉格朗日对偶性，有如下定理：

> **《统计学习方法》定理 C.2 (P. 450)**
>
> 考虑原始问题和对偶问题。假设函数 $f(x)$ 和 $c_i(x)$ 是凸函数，$h_j(x)$ 是仿射函数；并且假设不等式约束 $c_i(x)$ 是严格可行的，即存在 x，对所有 i 有 $c_i(x)<0$，则存在 $x^*$，$\alpha^*$，$\beta^*$，使 $x^*$ 是原始问题的解，$\alpha^*$，$\beta^*$ 是对偶问题的解，并且
>
> $$
> p^* = d^* = L(x^*,\alpha^*,\beta^*)p^* = d^* = L(x^*,\alpha^*,\beta^*)
> $$

根据上述定理，最大熵模型可以通过求解对偶问题来求解原始问题的条件如下：

1. $-H(P)=\sum_{x,y} \tilde{P}(x) P(y|x) log P(y|x)$ 是凸函数；
2. $E_p(f_i) - E_{\tilde{P}}(f_i) = 0$ 是仿射函数；
3. $\sum_y P(y|x) = 1$ 是仿射函数。

第 3 个条件显然成立，下面依次证明第 1 个条件和第 2 个条件。

在最大熵模型中，分类模型是条件概率分布 $P(Y|X)$，学习的目标是用最大熵原理选择最好的分类模型，因此自变量就是条件概率分布 $P(Y|X)$。根据训练数据集确定的联合分布的经验分布 $\tilde{P}(X,Y)$ 和边缘分布的经验分布 $\tilde{P}(X)$ 均应视为常量。

##### 证明第 1 个条件

求 $-H(P)$ 对 $P(y|x)$ 的二阶偏导数

$$
\frac{\partial (-H(P))}{\partial P(y|x)} = \sum_{x,y} \frac{\tilde{P}(x)}{P(y|x)}
$$

因为有 $\tilde{P}(x) = \frac{v(X=x)}{N} >= 0$，$P(y|x) >= 0$，所以二阶偏导数恒非负。于是，第 1 个条件得证。

##### 证明第 2 个条件

将 $E_p(f_i)$ 和 $E_{\tilde{P}}(f_i)$ 展开，于是有

$$
\begin{align}
E_p(f_i) - E_{\tilde{P}}(f_i)
& = \sum_{x,y} \tilde{P}(x) P(y|x) f(x,y) - \sum_{x,y} \tilde{P}(x,y) f(x,y) \\
& = \sum_{x,y} f(x,y) \big[ \tilde{P}(x) P(y|x) - \tilde{P}(x,y) \big]
\end{align}
$$

于是，$E_p(f_i) - E_{\tilde{P}}(f_i)$ 显然是关于 $P(Y|X)$ 的仿射函数。

#### 最大熵模型对数似然函数中指数的含义？

因为在训练数据集中，$(x,y)$ 的样本可能不止一个，而连乘符号中仅区分了不同的 $(x,y)$。如果没有指数的话，那么无论 $(x,y)$ 的出现频数是多少，均只乘了一次。

$\tilde{P}(x,y)$ 正是 $(x,y)$ 的样本的频数，因此使用 $\tilde{P}(x,y)$ 作为指数，以表示 $(x,y)$ 的样本需要连乘的次数。

