# 数据挖掘笔记2  

> @author：徐佳诚  

**支持向量机的类别划分**：

* 用于解决二类分类问题的监督学习模型
* 非概率模型
* 线性模型&非线性模型：线性支持向量机是线性模型，核函数支持向量机是非线性模型。
* 非参数化模型
* 判别模型

**支持向量机的主要优点**：

**支持向量机的主要缺点**：

---

【补充说明】合页损失函数在P. 131有详细讲解。

#### 希尔伯特空间

希尔伯特空间，即完备正交的线性空间。“完备”是指状态的所有可能值都被这个空间包含了。例如有理数集就是不完备的。“正交”是指各个坐标基相互垂直/正交。“线性”是指不同的量之间满足加法规则。希尔伯特空间可以是无穷维，也可以是有限维。

##  线性可分支持向量机

#### 支持向量机与感知机的关系


感知机和线性可分支持向量机都假设训练数据集是线性可分的。感知机利用误分类最小的策略，求得任意一个分离超平面，此时的解有无穷多个；线性可分支持向量机则利用硬间隔最大化求最优分离超平面，此时的解是唯一的。

## 函数间隔和几何间隔

#### 点到超平面距离公式的推导过程

【延伸阅读】[几何间隔为什么是离超平面最近的点到超平面的距离？ - 长行的回答 - 知乎](https://www.zhihu.com/question/30217705/answer/1942943891)

##  间隔最大化

【补充说明】在设$x_1'$、$x_1''$、$x_2'$和$x_2''$时，要使得不等式等号成立的不等式，是约束条件中的不等式（7.14）。

##  学习的对偶算法

#### 为什么$w^*=0$原始最优化问题的解？

因为根据数据集的线性可分性，存在$w·x+b=0$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例i，有$w·x_i+b>0$，对所有$y_i=-1$的实例i，有$w·x_i+b<0$。

用反证法，当$w=0$时，显然冲突。

##  线性支持向量机与软间隔最大化

【补充说明】本章节先给出了对偶问题的结果（7.37-7.39），然后才给出的对偶问题的推导过程。

【补充说明】KKT条件中关于$\xi$的偏导数，应写作$\nabla_\xi L(w^*,b^*,\xi^*,\alpha^*,\mu^*) = C-\alpha_i^*-\mu_i^* = 0$。

#### 软间隔的支持向量机中样本点的位置

* 若$\alpha_i^* = 0$：则有$\mu_i=C$，$\xi_i = 0$
  * 若$y_i(w^*·x_i+b^*) > 1$。分类正确，$x_i$在间隔边界以外。
  * 若$y_i(w^*·x_i+b^*) = 1$。分类正确，$x_i$恰好落在间隔边界上。（这种情况是否能够发生？）
* 若$0 < \alpha_i^* < C$：则有$0<\mu_i<C$，$\xi_i = 0$；于是$y_i(w^*·x_i+b^*) = 1$。分类正确，$x_i$恰好落在间隔边界上。
* 若$\alpha_i^* = C$：则有$\mu_i=0$，$y_i(w^*·x_i+b^*) = 1-\xi_i$
  * 若$\xi_i = 0$：于是$y_i(w^*·x_i+b^*)-1 = 0$。分类正确，$x_i$恰好落在间隔边界上。（这种情况是否能够发生？）
  * 若$0<\xi_i<1$：于是$0<y_i(w^*·x_i+b^*)<1$。分类正确，$x_i$在间隔边界与分离超平面之间。
  * 若$\xi_i=1$：于是$y_i(w^*·x_i+b^*) = 0$。$x_i$刚好落在分离超平面上。
  * 若$\xi_i>1$：于是$y_i(w^*·x_i+b^*) < 0$。分类错误，$x_i$在分离超平面的误分类一侧。

#### 线性支持向量机（sklearn实现）


```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC

if __name__ == "__main__":
    X, Y = load_breast_cancer(return_X_y=True)
    x1, x2, y1, y2 = train_test_split(X, Y, test_size=1 / 3, random_state=0)

    clf = LinearSVC()
    clf.fit(x1, y1)

    print("正确率:", clf.score(x2, y2))  # 正确率: 0.8368421052631579
```

## 核技巧

【重点】用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数聚中学习分类模型。核技巧就属于这样的方法。

#### 什么是核函数？

核函数隐含着一个从低维空间到高维（甚至是无穷维）空间的映射，我们几乎可以认为，这样能够将在低维中线性不可分的数据集在高维空间中找到线性可分的超平面。但是这个过程是隐式地在特征空间中进行的，不需要显式地定义特征空间和映射函数。

此外，核函数还简化了内积的计算，使得我们不需要显式地计算映射函数，而直接求出内积的值。



##  正定核

#### 正定矩阵和半正定矩阵


通过以上资料，我们可以知道对称矩阵、二次型矩阵、二次型的概念是等价的。

> **【定义】正定矩阵和负定矩阵（来自高等教育出版社的《线性代数》 P. 137）**
>
> 设二次型$f(x)=x^T A x$，如果对任何$x \ne 0$，都有$f(x)>0$（显然$f(0)=0$），则称$f$为正定二次型，并称对称矩阵$A$是正定的；如果对任何$x \ne 0$都有$f(x)<0$，则称$f$是负定二次型，并称对称矩阵$A$是负定的。

类似地，半正定矩阵定义为：如果对任何$x \ne 0$，都有$f(x)>=0$（显然$f(0)=0$），则称$f$为半正定二次型，并称对称矩阵$A$是半正定的。

在以上定义中，正定矩阵、半正定矩阵和负定矩阵都是定义在二次型的基础上的，因此一定是对称矩阵。

正定矩阵具有如下定理及推论：

> **【定理 1】（来自高等教育出版社的《线性代数》 P. 137）**
>
> $n$元二次型$f = x^T A x$为正定的充分必要条件是：它的标准型的$n$个系数全为正，即它的规范形的$n$个系数全为1，亦即它的正惯性质数等于$n$。

> **【推论】（来自高等教育出版社的《线性代数》 P. 137）**
>
> 对阵矩阵$A$为正定的充分必要条件是：$A$的特征值全为正。

> **【定理 2】（来自高等教育出版社的《线性代数》 P. 137）**
>
> 对称矩阵$A$为正定的充分必要条件是：$A$的各阶主子式都为正，即
> $$
> a_{11} > 0
> , \hspace{1em}
> \begin{vmatrix}
> a_{11} & a_{12} \\
> a_{21} & a_{22}
> \end{vmatrix} > 0
> , \hspace{1em}
> ...
> , \hspace{1em}
> \begin{vmatrix}
> a_{11} & \cdots & a_{1n} \\
> \vdots &        & \vdots \\
> a_{n1} & \cdots & a_{nn} 
> \end{vmatrix} > 0
> $$

#### 根据核函数构成希尔伯特空间的步骤解析

根据假设，我们已知 $K(x,z)$ 是定义在 $X×X$ 上的对称函数，并且对任意的 $x_1,x_2,\cdots,x_m \in X$，$K(x,z)$ 关于 $x_1,x_2,\cdots,x_m$ 的 Gram 矩阵是半正定的。

##### 1. 定义映射，构成向量空间 S

先定义映射

$$
\phi : x \rightarrow K(·,x) \tag{1}
$$

> 根据核函数的定义可知，核函数定义为 $K(x,z) = \phi(x)·\phi(z)$，从输入空间到特征空间的映射函数可以显式地表达为 $\phi : x \rightarrow \phi(x)$。但是在核技巧下，我们并不需要显式地定义映射函数，因此采用上式（1）隐式地定义映射函数。
>
> 为方便理解，也可以将上式不严谨地理解为：只代入了一个参数 $\phi(x)$ 的核函数，而 $·$ 表示尚未代入的另一个参数。

根据这一映射，对任意 $x_i \in X$，$\alpha_i \in R$，$i=1,2,\cdots,m$，定义线性组合

$$
f(·)=\sum_{i=1}^m \alpha_i K(·,x_i) \tag{2}
$$

> 于是，上式（2）就是对只代入一个参数 $\phi(x)$ 的核函数的线性变换结果（加法运算和数乘运算）。
>
> 为方便理解，也可以将上式不严谨地理解为：只代入了一个参数 $\phi(\sum_{i=1}^M \alpha_i x_i)$ 的核函数。
>
> 之所以要定义这个线性组合，是因为因为向量空间是带有加法和标量乘法的集合。通过线性组合的方式令包含的元素对加法和数乘运算封闭，从而使 $S$ 成为向量空间。

考虑由线性组合为元素的集合 S。由于集合 S 对加法和数乘运算是封闭的，所以 S 构成一个向量空间。
##  常用核函数

> 【补充说明】书中所描述的子串的概念，类似于通常所用的子序列的概念。通常子串是指连续的子序列，即书中拥有连续的$i$的子串。

#### 其他常用核函数

线性核函数：
$$
K(x,z) = x·z+c
$$
幂质数核函数：
$$
K(x,z) = exp(-\frac{||x-z||}{2 \sigma^2})
$$
拉普拉斯核函数：
$$
K(x,z) = exp(-\frac{||x-z||}{\sigma})
$$

#### 余弦相似度

余弦相似度的计算公式如下：
$$
similarity = cos(\theta) = \frac{A·B}{||A|| \ ||B||} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n (A_i)^2} × \sqrt{\sum_{i=1}^n (B_i)^2}}
$$
其中$A_i$和$B_i$分别为向量$A$和$B$中的特征。

#### 字符串核函数的动态规划计算

已知两个字符串 $s$ 和 $t$ 上的字符串核函数是基于映射 $\phi_n$ 的特征空间中的内积，即字符串 $s$ 和 $t$ 中长度等于 $n$ 的所有子串组成的特征向量的余弦相似度。其计算公式如下：

$$
\begin{align*}
k_n(s,t) & =\sum_{u \in \Sigma^n} [\phi_n(s)]_u [\phi_n(t)]_u \\
& =\sum_{u \in \Sigma^n} \sum_{(i,j):s(i)=t(j)=u} \lambda^{l(i)} \lambda^{l(j)}
\end{align*}
$$

其中 $\lambda$ 是衰减参数，$u$ 是相同子序列，$l(i)$ 为子序列在 $s$ 中的长度（最后 1 个字符的下标-第 1 个字符的下标+1），$l(j)$ 为子序列在 $t$ 中的长度。

观察上式，我们可以发现，每一个相同子串所提供的相似度，都是 $\lambda^{l(i)+l(j)}$，即衰减参数的两个子串的长度之和次方；于是有长度每增加 $1$，所提供的相似度都只需要乘以 $\lambda$ 即可。

于是，我们很自然地想到，可以用类似计数 DP 的方法，当前状态（$s1$ 的前 $i$ 个字符和 $s2$ 的前 $j$ 个字符），只需要记录所有长度为 $l$ 为相同子序列提供的相似度之和即可。当考虑下一个状态（例如 $s1$ 的前 $i+1$ 个字符和 $s2$ 的前 $j$ 个字符）时，只需要简单地将上一个状态乘以衰减因子 $\lambda$ 即可。需要注意的是，我们需要考虑可能存在的被重复计算的问题。

具体地，状态矩阵如下：

`dp[l][i]][j]`：$s1$ 的前 $i$ 个字符和 $s2$ 的前 $j$ 个字符中，所有长度为 $l$ 的相同子序列所提供的相似度的和。因为考虑到状态转移中，只会用到 $l$ 和 $l-1$，所以可以省略 $l$ 以节约空间。

状态转移方程如下（其中`att`为衰减因子）：

`dp[l][i][j] = dp[l][i-1][j] * att + dp[l][i][j-1] * att + dp[l][i-1][j-1] * att * att`（第 3 项是在处理被重复计算的问题）

在当前字符相同时，额外增加使用当前相同字符的情况：`dp[l][i][j] += dp[l-1][i-1][j-1] * att * att`；同时，当前子序列长度的核函数的结果由这部分累加即可。

```python
def count_kernel_function_for_string(s1, s2, length: int, att: float):
    """计算字串长度等于n的字符串核函数的值

    :param s1: 第1个字符串
    :param s2: 第2个字符串
    :param length: 需要查找的英文字符集长度
    :param att: 衰减参数
    :return: 字符串核函数的值
    """

    # 计算字符串长度
    n1, n2 = len(s1), len(s2)

    # 定义状态矩阵
    dp1 = [[1] * (n2 + 1) for _ in range(n1 + 1)]

    # 字符串的核函数的值的列表：ans[i] = 子串长度为(i+1)的字符串核函数的值
    ans = []

    # 依据子串长度进行状态转移：[1,n]
    for l in range(length):
        dp2 = [[0] * (n2 + 1) for _ in range(n1 + 1)]

        # 定义当前子串长度的核函数的值
        res = 0

        # 进行状态转移
        for i in range(1, n1 + 1):
            for j in range(1, n2 + 1):
                dp2[i][j] += dp2[i - 1][j] * att + dp2[i][j - 1] * att - dp2[i - 1][j - 1] * att * att
                if s1[i - 1] == s2[j - 1]:
                    dp2[i][j] += dp1[i - 1][j - 1] * att * att
                    res += dp1[i - 1][j - 1] * att * att  # 累加当前长度核函数的值

        dp1 = dp2
        ans.append(res)

    return ans[-1]
```

时间复杂度、空间复杂度分析：

- 时间复杂度：$O(N1×N2×Length)$；其中 $N1$ 为第 1 个字符串的长度，$N2$ 为第 2 个字符串的长度，$Length$ 为子串长度
- 空间复杂度：$O(N1×N2)$

#### 非线性支持向量机（sklearn实现）

```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

if __name__ == "__main__":
    X, Y = load_breast_cancer(return_X_y=True)
    x1, x2, y1, y2 = train_test_split(X, Y, test_size=1 / 3, random_state=0)

    clf = SVC(kernel="rbf")
    clf.fit(x1, y1)

    print("正确率:",clf.score(x2, y2))  # 正确率: 0.9263157894736842
```
